---
title: "SYS 6018 Semester Project Part 1"
author: "Camille Leonard"
output:
  html_document:
    df_print: paged
---

### **Project Introduction**  

The objective of this project is to create predictive models to accurately and quickly identify the locations of people in need of aid from data collected by Rochester Institute of Technology in the wake of the 2010 earthquake in Haiti. 

In part one of this project, logistic regression, LDA, QDA, and KNN methods will be used to create models. 

```{r, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 7,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "60%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

```{r 1, echo=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(MASS)
library(ggplot2)
library(GGally)
library(caret)
library(pROC)
library(class)
library(stats)
library(ROCR)
```

```{r 2}
df <- tibble(read.csv("HaitiPixels.csv")) #read in df
anyNA(df) #check for NA values 
summary(df) #quick look at data
ggpairs(df[,2:4], lower.panel = NULL) #view scatter and correlations
df$Class <- factor(df$Class) #make Class a factor variable. 
attach(df) #attach df variables 
```

```{r 3}
#take a look at the distribution of classes and RBG values 
featurePlot(x = df[,2:4],
            y = df$Class,
            plot = "box",
            layout = c(3,1), 
            scales = list(y = list(relation = "free"),
                          x = list(rot = 90)))
```

```{r 4}
df <- cbind(mutate(df, "Blue_Tarp_or_Not"=ifelse(Class != "Blue Tarp", 0, 1))) #add binary column indicating whether the Class variable is "Blue Tarp" or not
attach(df)
Blue_Tarp_or_Not <- as.factor(Blue_Tarp_or_Not) #ensure new column is a factor 
df
tail(df)

```


```{r 5}
#create training and final hold out set (called test)
set.seed(4)
trainIndex <- createDataPartition(Class, p=0.8,
                                  list=FALSE,
                                  times=1)

blueTarpTrain <- df[trainIndex,]
blueTarpTrain$Blue_Tarp_or_Not <- as.factor(blueTarpTrain$Blue_Tarp_or_Not)
blueTarpTrain <- blueTarpTrain[,-1] #remove Class column
blueTarpTest <- df[-trainIndex,]
blueTarpTest$Blue_Tarp_or_Not <- as.factor(blueTarpTest$Blue_Tarp_or_Not)
blueTarpTest <- blueTarpTest[,-1] #remove Class column

```


## Logistic Regression 

```{r 6, message=FALSE, warning=FALSE}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10) #set fitControl parameters such that 10 folds and 10 repeats are done

set.seed(4)
glm.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    method="glm",
                    family="binomial",
                    trControl= fitControl)

glm.fit

summary(glm.fit)
```

```{r}
glm.prob <- predict(glm.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
glm_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, glm.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="GLM ROC Curve") 
```

```{r}
roc.info_glm <- roc(blueTarpTrain$Blue_Tarp_or_Not, glm.prob[,2], legacy.axes=TRUE)
roc.glm.df <- data.frame(tpp=roc.info_glm$sensitivities*100, fpp=(1-roc.info_glm$specificities)*100, thresholds=roc.info_glm$thresholds)
roc.glm.df[roc.glm.df>98.5 & roc.glm.df < 99,]
```

```{r}
glm.pred_50 <- ifelse(glm.prob[,2]>0.5,1,0)
cm.glm_50 <- confusionMatrix(factor(glm.pred_50), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.5"
cm.glm_50 

glm.pred_15 <- ifelse(glm.prob[,2]>0.15,1,0)
cm.glm_15 <- confusionMatrix(factor(glm.pred_15), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.15"
cm.glm_15

```

## LDA 

```{r 10}

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

set.seed(4)
lda.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="lda",
                    verbose= FALSE,
                    trControl= fitControl)

lda.fit
#summary(lda.fit)
```

```{r}
lda.prob <- predict(lda.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
lda_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, lda.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="LDA ROC Curve") 
```
```{r}
roc.info_lda <- roc(blueTarpTrain$Blue_Tarp_or_Not, lda.prob[,2], legacy.axes=TRUE)
roc.lda.df <- data.frame(tpp=roc.info_lda$sensitivities*100, fpp=(1-roc.info_lda$specificities)*100, thresholds=roc.info_lda$thresholds)
roc.lda.df[roc.lda.df>91.5 & roc.lda.df < 91.6,]
```

```{r}
lda.pred_50 <- ifelse(lda.prob[,2]>0.5,1,0)
cm.lda_50 <- confusionMatrix(factor(lda.pred_50), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.5"
cm.lda_50 

lda.pred_10 <- ifelse(lda.prob[,2]>0.10,1,0)
cm.lda_10 <- confusionMatrix(factor(lda.pred_10), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.10"
cm.lda_10
```

## QDA 

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

set.seed(4)
qda.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="qda",
                    verbose= FALSE,
                    trControl= fitControl)

qda.fit

```

```{r}
qda.prob <- predict(qda.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
qda_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, qda.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="QDA ROC Curve") 
```
```{r}
roc.info_qda <- roc(blueTarpTrain$Blue_Tarp_or_Not, qda.prob[,2], legacy.axes=TRUE)
roc.qda.df <- data.frame(tpp=roc.info_qda$sensitivities*100, fpp=(1-roc.info_qda$specificities)*100, thresholds=roc.info_qda$thresholds)
roc.qda.df[roc.qda.df>98 & roc.qda.df < 99,]
```

```{r}
qda.pred_50 <- ifelse(qda.prob[,2]>0.5,1,0)
cm.qda_50 <- confusionMatrix(factor(qda.pred_50), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.5"
cm.qda_50 

qda.pred_10 <- ifelse(qda.prob[,2]>0.10,1,0)
cm.qda_10 <- confusionMatrix(factor(qda.pred_10), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.10"
cm.qda_10

```




## KNN
```{r cores, echo=FALSE}
cores <- parallel::detectCores()
cores
```
```{r}
all_cores <- parallel::detectCores(logical = FALSE)
all_cores
```
```{r}
library(doParallel)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```
```{r}
library(tune)
grid_control <- control_grid(verbose = TRUE,pkgs = "doParallel",allow_par = TRUE)
```

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10, #number of folds
                           repeats = 10) # number of times each fold is run; can the number of repeats be reduced? do both of these need to be 10?

set.seed(4)
knn.fit <- train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="knn",
                    trControl= fitControl,
                    tuneLength=10
                    )

knn.fit


```
```{r, echo=FALSE}
parallel::stopCluster(cl)
```

```{r}
knn.prob <- predict(knn.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
knn_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, knn.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="KNN ROC Curve") 
```
```{r}
roc.info_knn <- roc(blueTarpTrain$Blue_Tarp_or_Not, knn.prob[,2], legacy.axes=TRUE)
roc.knn.df <- data.frame(tpp=roc.info_knn$sensitivities*100, fpp=(1-roc.info_knn$specificities)*100, thresholds=roc.info_knn$thresholds)
#roc.knn.df[roc.knn.df>99 & roc.knn.df < 100,]
roc.knn.df
```

```{r}
knn.pred_50 <- ifelse(knn.prob[,2]>0.5,1,0)
cm.knn_50 <- confusionMatrix(factor(knn.pred_50), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.5"
cm.knn_50 

knn.pred_077 <- ifelse(knn.prob[,2]>0.077,1,0)
cm.knn_077 <- confusionMatrix(factor(knn.pred_077), factor(blueTarpTrain$Blue_Tarp_or_Not), positive = '1') 
"Threshold: 0.077"
cm.knn_077
```


## Final Hold Out Performance 

GLM
```{r}
glm.prob_FHO <- predict(glm.fit, newdata=blueTarpTest, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
glm.pred_FHO <- ifelse(glm.prob_FHO[,2]>0.15,1,0)
cm.glm_50_FHO <- confusionMatrix(factor(glm.pred_FHO), factor(blueTarpTest$Blue_Tarp_or_Not), positive = '1') 
cm.glm_50_FHO
```

LDA
```{r}
lda.prob_FHO <- predict(lda.fit, newdata =blueTarpTest, type = "prob")
lda.pred_FHO <- ifelse(lda.prob_FHO[,2]> 0.10,1,0)
cm.lda_FHO <- confusionMatrix(factor(lda.pred_FHO), factor(blueTarpTest$Blue_Tarp_or_Not), positive="1")
cm.lda_FHO
```

QDA 

```{r}
qda.prob_FHO <- predict(qda.fit, newdata =blueTarpTest, type = "prob")
qda.pred_FHO <- ifelse(qda.prob_FHO[,2]>0.10,1,0)
cm.qda_FHO <- confusionMatrix(factor(qda.pred_FHO), factor(blueTarpTest$Blue_Tarp_or_Not), positive="1")
cm.qda_FHO
```

KNN
```{r}
knn.prob_FHO <- predict(knn.fit, newdata =blueTarpTest, type = "prob")
knn.pred_FHO <- ifelse(knn.prob_FHO[,2]>0.077,1,0)
cm.knn_FHO <- confusionMatrix(factor(knn.pred_FHO), factor(blueTarpTest$Blue_Tarp_or_Not), positive="1")
cm.knn_FHO
```



## Results Table 

|                   Method | KNN (k = 5)  |    LDA    |    QDA    | Log. Regression |
|-------------------------:|:------------:|:---------:|:---------:|:---------------:|
|                 Accuracy | 99.6%        | 98.2%     | 98.9%     | 99.2%           |
|                      AUC | 100%         | 98.9%     | 99.8%     | 99.8%           |
|                      ROC |              |           |           |                 |
|                Threshold | 0.077        | 0.10      | 0.10      | 0.15            |
| Sensitivity=Recall=Power | 98.8%        | 84.7%     | 90.0%     | 94.6%           |
|        Specificity=1-FPR | 99.7%        | 98.7%     | 99.3%     | 99.3%           |
|                      FDR | 9.6%         | 32.3%     | 19.6%     | 18.3%           |
|            Precision=PPV | 90.4%        | 67.3%     | 80.4%     | 81.7%           |



#### Threshold Justifications

Assuming that we have sufficient resources (personnel and the physical resources) to provide food and water for any blue tarps we find, our main priority is finding blue tarps and minimizing not finding blue tarps. The cost of human life is one that is impossible to truly quantify. The cost of potential resource waste due to false positives is worth the potential of saving a life. Due to the imbalanced nature of the data set we also need to keep in mind the proportion of false positives (FDR) compared to the proportion of TP. Therefore, we want predict as many TP as we can while keeping the precision high. It is alright to have some false positives but we don't want them to be a large percentage of our predicted positives.   



##### KNN 

The KNN model performs well with 99.6% accuracy and 100% AUC. Based on the ROC graph, a threshold of 0.077 results in a sensitivity of 98.8% and 0.3% FPR. The FDR for this threshold is 9.6% the best of all the models. The KNN model with a 0.077 threshold achieves our objective of predicting a large TP% with a low FDR.  
 

#### LDA 

The LDA model, with a 98.2% accuracy and 98.9% AUC, does not perform as well as the KNN model. It was difficult to choose a threshold for this model and 0.10 was determined to have a reasonable trade off between FP and FN. A threshold of 0.10 results in a sensitivity of 84.7% and 1.3% FPR. The FDR for this threshold is 32.3%.  This model does not perform as well as the KNN model and has and FDR that is too high. 


#### QDA 

The QDA model, with a 98.9% accuracy and 99.8% AUC, does not perform as well as the KNN model. A threshold of 0.10 was chosen because it had a reasonable trade off between FP and FN. A threshold of 0.026 produced a sensitivity of 90.0% and FPR of 0.7%. The FDR was 19.6% which was higher than would be preferred for the intended application. 
 

#### Logistic Regression 

The logistic regression model, with a 99.2% accuracy and 99.8% AUC, performs well. A threshold of 0.15 was selected which resulted in a sensitivity of 94.6% and 0.7% FPR. The FDR was 18.3%, which is still higher than would be preferred. This FDR is second best of the four models. 


## Conclusions 

Without a doubt, the KNN method performs the best with the highest accuracy, highest AUC, and highest precision (lowest FDR).The rest of the models, in order of best performance, are: Logistic Regression, QDA, and LDA. The final models when tested against the final hold out set performed well. Ordered from best to worst performing: KNN, logistic regression, QDA and LDA. These results support my previous conclusions.

Based on the previously outlined metrics, the KNN and logistic regression models are the two best of the four models created for this project.  However, because time and accuracy are of the essence I would recommend that the logistic regression model be used. There will be more false positives classified by the logistic regression method compared to the KNN method. In our scenario, we have the personnel and resources to cover extra false positives. What we don't have is extra time waiting for our classification system to return results of where to find the blue tarps.  

I think the results of this project would be immensely successful in saving human lives. Any of these methods are much faster than a human scouring thousands of images looking for blue dots. These methods scale well and with the access to computing power that exists today any amount of data that were collected could be processed.  

As a general strategy, I would recommend that each supply mission target areas that have the highest clusters of predicted blue tarps. That way if false positives are found the supply team can continue on to the next location and not have to spend as much time traveling from location to location.  

To improve these results, RBG values could be converted into a singular color designation like a hexcode. Then an even better performing classification method like a Bayes decision boundary could be used to determine the line between colors that should be considered blue and indicate blue tarps and those that do not.  Other ideas would be to add other forms of data that indicate variables other than color, like infrared or thermal imaging from satellites to detect people. People detected could then be cross referenced with areas where tarps might be located. 

I think one thing that made it possible to use predictive models is that the object we were looking for really stood out in comparison to the environment that it was in. If the tarps had been brown or even red it would have been far more challenging to effectively classify it as a tarp and not some kind of vegetation or dirt. If the tarp had been a different color it's possible these methods might not have worked at all and other data would have been needed to do any classification.  

#### References 

* Caret Documentation: https://topepo.github.io/caret/index.html 
* ROCR Documentation: https://www.rdocumentation.org/packages/ROCR/versions/1.0-11 
* My group's STAT 6021 Final Project
* article about colorized ROC curves 
* https://davidrroberts.wordpress.com/2015/09/22/quick-auc-function-in-r-with-rocr-package/ 
* https://www.youtube.com/watch?v=qcvAqAH60Yw&ab_channel=StatQuestwithJoshStarmer
* https://towardsdatascience.com/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a#e16f
* https://towardsdatascience.com/demystifying-roc-curves-df809474529a
* https://mlr.mlr-org.com/articles/tutorial/roc_analysis.html
* https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5
* Thanks to Derek for sharing code for doing multicore processing
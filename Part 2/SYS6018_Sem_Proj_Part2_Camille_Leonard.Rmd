---
title: "SYS 6018 Semester Project Part 2"
author: "Camille Leonard"
date: "10/31/2020"
output: 
  html_document:
    theme: spacelab 
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: true 
      smooth_scroll: false 
    number_sections: true
    fig_caption: true 
    code_folding: hide  
    
    
---
# Project Introduction 

On [January 12, 2010](en.wikipedia.org/wiki/2010_Haiti_earthquake), a magnitude 7.0 earthquake struck Haiti causing significant damage which affected approximately 3 million citizens. Approximately 1.5 million of the affected citizens were forced to live in makeshift shelters and camps. In the wake of the disaster, aid groups were working to locate displaced persons and provide them with food and water. However, due to the large scale destruction of infrastructure over a wide area additional assistance was needed to locate people quickly. 

Displaced persons were known to be making make-shift shelters out of blue tarps. High resolution geo-refereneced images were captured by aircraft of the destroyed areas. The data generated by the image collection was too large for aid workers to process in time to supply aid. Therefore, a team from the [Rochester Institute of Technology](NEED LINK) used data-mining algorithms to analyze the images and identify blue tarps. The goal was to effectively locate displaced persons and communicate their location to rescue workers so they could get resources to people who needed it in time.  

As the final project for SYS 6018 - Data Mining, we were assigned to build models from the different techniques we learned in the course to, as accurately as possible, and in as timely a manner as possible, locate as many of the displaced persons identified in the imagery data so that they could be provided food and water before their situations became unsurvivable.    

# Risk Analysis / Cost Benefit / Budget for Project 
The US Government spent [$1.5B on Haiti disaster relief](https://www.rand.org/pubs/research_reports/RR304.html) by the end of 2010.  
*method of delivery
*cost of resources
*time from disaster to unsurvivable conditions 

# Analysis Methods 

```{r, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 7,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "60%", # set width of displayed images
                      warning=FALSE,     # do not show R warnings
                      message=FALSE)     # do not show R messages
```

```{r 1, echo=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(MASS)
library(ggplot2)
library(GGally)
library(caret)
library(pROC)
library(class)
library(stats)
library(ROCR)
library(randomForest)
library(e1071)
```

```{r cores, echo=FALSE}
cores <- parallel::detectCores()
cores
```
```{r}
all_cores <- parallel::detectCores(logical = FALSE)
all_cores
```
```{r}
library(doParallel)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```
```{r}
library(tune)
grid_control <- control_grid(verbose = TRUE,pkgs = "doParallel",allow_par = TRUE)
```

```{r}
#read in the data set and wrangle 

```


## EDA 
```{r}
#populate this section with tidy / ggplot exploration... do more than you did last time 
#consider doing a 3d scatterplot with plotly... 
```

## Prepare Data Frame for Analysis 
Include an explainer... 
```{r}
#add binary column for blue tarp not blue tarp 
#split into test and train set
```


## Logistic Regression 
Fit a Logistic Regression Model 
!!!Need to turn on the fold result saving ... 
```{r 6, message=FALSE, warning=FALSE}
pass 
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10) #set fitControl parameters such that 10 folds and 10 repeats are done

set.seed(4)
glm.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    method="glm",
                    family="binomial",
                    trControl= fitControl)

glm.fit

summary(glm.fit)
```


Test model performance on Train data to select threshold values... 
```{r}
pass
glm.prob <- predict(glm.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
glm_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, glm.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="GLM ROC Curve") 
```

## LDA 

```{r 10}
pass
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

set.seed(4)
lda.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="lda",
                    verbose= FALSE,
                    trControl= fitControl)

lda.fit
#summary(lda.fit)
```


```{r}
lda.prob <- predict(lda.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
lda_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, lda.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="LDA ROC Curve") 
```


## QDA 

```{r}
pass
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

set.seed(4)
qda.fit <- caret::train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="qda",
                    verbose= FALSE,
                    trControl= fitControl)

qda.fit

```

```{r}
pass
qda.prob <- predict(qda.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
qda_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, qda.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="QDA ROC Curve") 
```

## KNN 

```{r}
pass
fitControl <- trainControl(method = "repeatedcv",
                           number = 10, #number of folds
                           repeats = 10) # number of times each fold is run; can the number of repeats be reduced? do both of these need to be 10?

set.seed(4)
knn.fit <- train(Blue_Tarp_or_Not~Red+Green+Blue,
                    data = blueTarpTrain,
                    preProcess=c("center","scale"),
                    method="knn",
                    trControl= fitControl,
                    tuneLength=10
                    )

knn.fit


```

```{r}
pass
knn.prob <- predict(knn.fit, newdata=blueTarpTrain, type = "prob") #returns df with col 0 (prob not blue tarp) and 1 (prob blue tarp)
par(pty="s")
knn_roc <- roc(blueTarpTrain$Blue_Tarp_or_Not, knn.prob[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Positive Percentage", col="#965fd4", lwd=4, print.auc=TRUE, main="KNN ROC Curve") 
```

## Random Forest 

## SVM

## K-Folds Out of Sampling Performance 

### Table 2 - Performance Metrics: 10-Fold Cross-Validation Metrics 

!!!!! THESE VALUES HAVE NOT BEEN UPDATED, POSSIBLY CONSIDER AUTOMATING THEM  

|                   Method | KNN (k = 5)  |    LDA    |    QDA    | Log. Regression | Random Forest (tuning param = ?) | SVM (tuning param = ?)|
|-------------------------:|:------------:|:---------:|:---------:|:---------------:|:--------------------------------:|:---------------------:|
|                 Accuracy | 99.6%        | 98.2%     | 98.9%     | 99.2%           | | |
|                      AUC | 100%         | 98.9%     | 99.8%     | 99.8%           | | |
|                      ROC |              |           |           |                 | | |
|                Threshold | 0.077        | 0.10      | 0.10      | 0.15            | | |
| Sensitivity=Recall=Power | 98.8%        | 84.7%     | 90.0%     | 94.6%           | | |
|        Specificity=1-FPR | 99.7%        | 98.7%     | 99.3%     | 99.3%           | | |
|                      FDR | 9.6%         | 32.3%     | 19.6%     | 18.3%           | | |
|            Precision=PPV | 90.4%        | 67.3%     | 80.4%     | 81.7%           | | |

## Model Performance 

```{r, echo=FALSE}
parallel::stopCluster(cl)
```
### Discussion 
(discussion on FHO data why we do this... what the benefits are... potential pitfalls)

(discussion somewhere about ROC curves AUC and... other metrics)

## Hold-Out Test Sample Performance 

### Table 3 - Performance Metrics: Hold-Out Test Data Set Scores  

!!!!! THESE VALUES HAVE NOT BEEN UPDATED, POSSIBLY CONSIDER AUTOMATING THEM  

|                   Method | KNN (k = 5)  |    LDA    |    QDA    | Log. Regression | Random Forest (tuning param = ?) | SVM (tuning param = ?)|
|-------------------------:|:------------:|:---------:|:---------:|:---------------:|:--------------------------------:|:---------------------:|
|                 Accuracy | 99.6%        | 98.2%     | 98.9%     | 99.2%           | | |
|                      AUC | 100%         | 98.9%     | 99.8%     | 99.8%           | | |
|                      ROC |              |           |           |                 | | |
|                Threshold | 0.077        | 0.10      | 0.10      | 0.15            | | |
| Sensitivity=Recall=Power | 98.8%        | 84.7%     | 90.0%     | 94.6%           | | |
|        Specificity=1-FPR | 99.7%        | 98.7%     | 99.3%     | 99.3%           | | |
|                      FDR | 9.6%         | 32.3%     | 19.6%     | 18.3%           | | |
|            Precision=PPV | 90.4%        | 67.3%     | 80.4%     | 81.7%           | | |


## Future Work 
```{r}
#consider if I was able to find an additional data source like lidar or infrared to pair with this dataset to improve model performance... ? 
```

# References and Works Cited 

* Thanks to Derek Banks for sharing code for doing multicore processing


# Appendix A: Analysis Methods Reference Info





